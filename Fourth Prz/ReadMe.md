# Практика 4: Атака DeepFool на модели ИИ


## Цель задания:

Изучить атаку DeepFool, которая предназначена для минимальных изменений в изображениях с
целью изменения их классификации. Научиться использовать эту атаку и исследовать влияние
противоречивых примеров на обученные модели.

## Задачи:

1. Загрузить ранее обученную модель на датасете MNIST.
2. Изучить теоретические основы атаки DeepFool.
3. Реализовать атаку DeepFool с помощью фреймворка Foolbox.
4. Оценить точность модели на противоречивых примерах и сравнить с результатами на обычных данных.

## Выполнение:

Выполнение работы было произведено в Google Colab - https://drive.google.com/file/d/17TstR1LM_kCaHSSlkwAeX4TfaD9WFwCr/view?usp=sharing

## Вывод:

В ходе выполнения практической работы была изучена атака DeepFool, предназначенная для минимального изменения изображений с целью введения модели в заблуждение. Была реализована атака с использованием библиотеки Foolbox и проведён анализ влияния созданных противоречивых примеров на точность предварительно обученной модели, использующей данные MNIST.

На основании итоговых результатов тестирования, точность модели на противоречивых примерах, созданных с помощью атаки DeepFool, составила 0%. Это свидетельствует о высокой эффективности атаки в контексте создания минимальных изменений, достаточных для сбоя классификации, и демонстрирует уязвимость модели перед подобными методами.

Таким образом, выполнение данной практической работы позволило глубже понять принципы работы атак на модели искусственного интеллекта, такие как DeepFool, а также оценить их влияние на производительность системы. Эти результаты подчеркивают необходимость разработки механизмов защиты от подобных атак для повышения устойчивости моделей ИИ.

## Автор

Брестер Андрей Николаевич, группа ББМО-02-23
