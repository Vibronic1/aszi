# Практика 5: Атака с ограниченной памятью (PGD -Projected Gradient Descent)


## Цель задания:

Изучить одну из наиболее мощных атак на модели ИИ — атаку Projected Gradient Descent (PGD).
Научиться использовать PGD для создания противоречивых примеров и оценить её влияние на обученные модели.

## Задачи:

1. Загрузить ранее обученную модель на датасете MNIST.
2. Изучить теоретические основы атаки PGD.
3. Реализовать атаку PGD с помощью фреймворка Foolbox.
4. Оценить точность модели на противоречивых примерах и сравнить с результатами на обычных данных.


## Выполнение:

Выполнение работы было произведено в Google Colab - https://drive.google.com/file/d/1WbU9TOcThBR5m-vrx8vu9crlL8cyd97P/view?usp=sharing

## Вывод:

Проведение атаки Projected Gradient Descent (PGD) в практическом задании позволило продемонстрировать уязвимость моделей машинного обучения к противоречивым примерам. Тестирование модели на обычных данных показало высокую точность — 97.15% на этапе обучения и 97.55% на тестовых изображениях, что подчеркивает качество её обучения и способность распознавать образы в стандартных условиях.

Однако, после применения атаки PGD точность модели значительно снизилась до 6.54% на этапе обучения и 8.79% на тестовых данных, что демонстрирует критическое воздействие атаки. Атака PGD, несмотря на минимальные изменения в данных, успешно обходит защитные механизмы модели, заставляя её ошибаться в большинстве случаев. Это подтверждает, что даже минимальные искажения входных данных могут полностью подорвать производительность модели, если такие изменения направлены на увеличение её ошибки.

Полученные результаты подчеркивают важность изучения и разработки методов защиты от атак, таких как PGD, для повышения устойчивости моделей. В будущем стоит исследовать подходы, направленные на увеличение устойчивости к противоречивым примерам, например, с использованием методов обучения с учетом атак или регуляризации.

## Автор

Брестер Андрей Николаевич, группа ББМО-02-23
