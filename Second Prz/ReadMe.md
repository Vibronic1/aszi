# Практика 2: Исследование атак на модели ИИ. Fast Gradient Sign Method (FGSM)


## Цель задания:

Познакомиться с одной из популярных атак на системы машинного обучения — атакой Fast Gradient
Sign Method (FGSM). Задача — научиться использовать FGSM для создания противоречивых (adversarial)
примеров, которые могут ввести обученную модель в заблуждение.

## Задачи:

1. Загрузить ранее обученную модель на датасете MNIST.
2. Изучить теоретические основы FGSM.
3. Реализовать атаку FGSM и сгенерировать противоречивые примеры.
4. Оценить точность модели на противоречивых примерах и сравнить с результатами на обычных данных.

## Выполнение:

Выполнение работы было произведено в Google Colab - https://drive.google.com/file/d/10iW8ZRJgrwRuT5mzuc1vvNQvwLDphpvp/view?usp=sharing

## Вывод:

В ходе выполнения задания исследовалась атака Fast Gradient Sign Method (FGSM) на модели машинного обучения. Используемая величина шума (параметр epsilon) составляла 0.1, что определяет интенсивность вносимых изменений в исходные данные. Атака проводилась на модели, обученной на датасете MNIST, с целью создания противоречивых примеров, которые могут вводить модель в заблуждение.

До применения FGSM модель показывала точность 97% при предсказаниях на тестовых данных. Однако, после генерации противоречивых примеров точность модели резко упала до 9%. Это показывает, насколько модель уязвима к небольшим преднамеренным изменениям во входных данных, которые направлены на увеличение ошибки предсказания. Падение точности подтверждает, что FGSM эффективно создаёт вводящие в заблуждение примеры.

Эти результаты подчёркивают важность изучения устойчивости моделей к атакам, особенно в критически важных областях. Разработка механизмов защиты от таких атак, например, методов повышения устойчивости к противоречивым примерам, становится ключевой задачей для обеспечения надёжности современных систем машинного обучения.

## Автор

Брестер Андрей Николаевич, группа ББМО-02-23
